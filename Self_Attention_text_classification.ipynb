{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self-Attention Text Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Data Check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- NSMC 데이터\n",
    "    - Naver sentiment movie corpus v1.0\n",
    "    - 네이버 영화 댓글 감정분석 데이터셋\n",
    "    - 이진 분류\n",
    "- https://github.com/e9t/nsmc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>document</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9976970</td>\n",
       "      <td>아 더빙.. 진짜 짜증나네요 목소리</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3819312</td>\n",
       "      <td>흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                           document  label\n",
       "0  9976970                아 더빙.. 진짜 짜증나네요 목소리      0\n",
       "1  3819312  흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나      1"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('data/ratings_train.txt', sep='\\t')\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "0    75173\n",
       "1    74827\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id          0\n",
       "document    5\n",
       "label       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(149995, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.dropna(how='any', axis=0)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = set()\n",
    "for doc in df['document']:\n",
    "    for token in doc.split():\n",
    "        vocab.add(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "357862"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_cnt_dict = {}\n",
    "for doc in df['document']:\n",
    "    for token in doc.split():\n",
    "        if token not in vocab_cnt_dict:\n",
    "            vocab_cnt_dict[token] = 0\n",
    "        vocab_cnt_dict[token] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_cnt_list = [(token, cnt) for token, cnt in vocab_cnt_dict.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_vocabs = sorted(vocab_cnt_list, key= lambda tup: tup[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnts = [cnt for _, cnt in top_vocabs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_vocab = sum(np.array(cnts)>=3)\n",
    "top_vocabs = top_vocabs[:n_vocab]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['영화', '너무', '정말', '진짜', '이']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabs = [token for token, _ in top_vocabs]\n",
    "vocabs[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## UNK, PAD Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabs.insert(0, '[UNK]')\n",
    "vocabs.insert(0, '[PAD]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_to_token = vocabs\n",
    "token_to_idx = {token: i for i, token in enumerate(idx_to_token)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    \n",
    "    def __init__(self, vocabs, use_padding=True, max_padding=64, pad_token='[PAD]', unk_token='[UNK]'):\n",
    "        self.idx_to_token = vocabs\n",
    "        self.token_to_idx = {token: i for i, token in enumerate(self.idx_to_token)}\n",
    "        \n",
    "        self.use_padding = use_padding\n",
    "        self.max_padding = max_padding\n",
    "        \n",
    "        self.pad_token = pad_token\n",
    "        self.unk_token = unk_token\n",
    "        \n",
    "        self.unk_token_idx = self.token_to_idx[self.unk_token]\n",
    "        self.pad_token_idx = self.token_to_idx[self.pad_token]\n",
    "        \n",
    "    def __call__(self, x:str):\n",
    "        token_ids = []\n",
    "        token_list = x.split()\n",
    "        \n",
    "        for token in token_list:\n",
    "            if token in self.token_to_idx:\n",
    "                token_idx = self.token_to_idx[token]\n",
    "            else:\n",
    "                token_idx = self.unk_token_idx\n",
    "            token_ids.append(token_idx)\n",
    "        \n",
    "        if self.use_padding:\n",
    "            token_ids = token_ids[:self.max_padding]\n",
    "            n_pads = self.max_padding - len(token_ids)\n",
    "            token_ids = token_ids + [self.pad_token_idx] * n_pads\n",
    "        \n",
    "        return token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(vocabs, use_padding=True, max_padding=50, pad_token='[PAD]', unk_token='[UNK]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_valid_df = pd.read_csv('data/ratings_train.txt', sep='\\t')\n",
    "test_df = pd.read_csv('data/ratings_test.txt', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150000, 50000)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_valid_df), len(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_valid_df = train_valid_df.sample(frac=1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratio = 0.8\n",
    "n_train = int(len(train_valid_df) * train_ratio)\n",
    "\n",
    "train_df = train_valid_df[:n_train]\n",
    "valid_df = train_valid_df[n_train:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(120000, 30000, 50000)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_df), len(valid_df), len(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1/10 샘플링\n",
    "train_df = train_df.sample(frac=0.1)\n",
    "valid_df = valid_df.sample(frac=0.1)\n",
    "test_df = test_df.sample(frac=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NSMCDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, data_df, tokenizer=None):\n",
    "        self.data_df = data_df\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data_df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample_raw = self.data_df.iloc[idx]\n",
    "        sample = {}\n",
    "        \n",
    "        sample['doc'] = str(sample_raw['document'])\n",
    "        sample['label'] = int(sample_raw['label'])\n",
    "        \n",
    "        assert sample['label'] in set([0, 1])\n",
    "        \n",
    "        if self.tokenizer is not None:\n",
    "            sample['doc_ids'] = self.tokenizer(sample['doc']) \n",
    "        \n",
    "        return sample        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    keys = [key for key in batch[0].keys()]\n",
    "    data = {key: [] for key in keys}\n",
    "\n",
    "    for item in batch:\n",
    "        for key in keys:\n",
    "            data[key].append(item[key])\n",
    "            \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = NSMCDataset(\n",
    "    data_df=train_df,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "valid_dataset = NSMCDataset(\n",
    "    data_df=valid_df,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "test_dataset = NSMCDataset(\n",
    "    data_df=test_df,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset,\n",
    "                              batch_size=64,\n",
    "                              collate_fn=collate_fn,\n",
    "                              shuffle=True)\n",
    "valid_dataloader = DataLoader(valid_dataset,\n",
    "                              batch_size=64,\n",
    "                              collate_fn=collate_fn,\n",
    "                              shuffle=False)\n",
    "test_dataloader = DataLoader(test_dataset,\n",
    "                             batch_size=64,\n",
    "                             collate_fn=collate_fn,\n",
    "                             shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = next(iter(test_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['doc', 'label', 'doc_ids'])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'벌써 세번째 감상중'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample['doc'][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1014, 9074, 1, 0, 0, 0, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample['doc_ids'][2][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Self-Attention Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 vocab_size,\n",
    "                 embed_dim):\n",
    "\n",
    "        super().__init__()\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_dim = embed_dim\n",
    "        self.output_dim = embed_dim\n",
    "\n",
    "        self.embeddings = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "\n",
    "        self.q_linear = nn.Linear(embed_dim, embed_dim)\n",
    "        self.k_linear = nn.Linear(embed_dim, embed_dim)\n",
    "        self.v_linear = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "    def forward(self, X, return_attention_score=False):\n",
    "\n",
    "        batch_size, seq_len = X.size()\n",
    "        X = self.embeddings(X)\n",
    "        \n",
    "        q, k, v = self.q_linear(X), self.k_linear(X), self.v_linear(X)\n",
    "        \n",
    "        attention_score_raw = q @ k.transpose(-2,-1) / math.sqrt(self.embed_dim)\n",
    "        \n",
    "        attention_score = torch.softmax(attention_score_raw, dim=2)\n",
    "        \n",
    "        weighted_sum = attention_score @ v\n",
    "\n",
    "        context = torch.mean(weighted_sum, dim=1)\n",
    "        \n",
    "        if return_attention_score:\n",
    "            return context, attention_score\n",
    "        \n",
    "        return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "\n",
    "    def __init__(self, sr_model, output_dim, vocab_size, embed_dim, **kwargs):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.sr_model = sr_model(vocab_size=vocab_size,\n",
    "                                 embed_dim=embed_dim,\n",
    "                                 **kwargs)\n",
    "\n",
    "        self.input_dim = self.sr_model.output_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.fc = nn.Linear(self.input_dim, self.output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(self.sr_model(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Classifier(sr_model=SelfAttention,\n",
    "                   output_dim=2,\n",
    "                   vocab_size=len(vocabs),\n",
    "                   embed_dim=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.sr_model.embeddings.weight[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = True and torch.cuda.is_available()\n",
    "\n",
    "if use_cuda:\n",
    "    model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(params=model.parameters(), lr=0.01)\n",
    "calc_loss = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global_i: 0, epoch_i: 0, loss: 0.7187510132789612\n",
      "******************************\n",
      "valid_loss_mean: 0.6035824697068397\n",
      "******************************\n",
      "******************************\n",
      "valid_loss_mean: 0.5936969138206319\n",
      "******************************\n",
      "******************************\n",
      "valid_loss_mean: 0.6519526744142492\n",
      "******************************\n",
      "******************************\n",
      "valid_loss_mean: 0.7940343428165355\n",
      "******************************\n",
      "******************************\n",
      "valid_loss_mean: 0.9876124662287692\n",
      "******************************\n",
      "best_epoch_i: 1, best_global_i: 940\n"
     ]
    }
   ],
   "source": [
    "n_epoch = 5\n",
    "global_i = 0\n",
    "\n",
    "valid_loss_hist = []\n",
    "train_loss_hist = []\n",
    "\n",
    "min_valid_loss = 9e+9\n",
    "best_model = None\n",
    "best_epoch_i = None\n",
    "\n",
    "for epoch_i in range(n_epoch):\n",
    "    model.train()\n",
    "    \n",
    "    for batch in train_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        x = torch.tensor(batch['doc_ids'])\n",
    "        y = torch.tensor(batch['label'])\n",
    "        \n",
    "        if use_cuda:\n",
    "            x = x.cuda()\n",
    "            y = y.cuda()\n",
    "            \n",
    "        y_pred = model(x)\n",
    "        loss = calc_loss(y_pred, y)\n",
    "        \n",
    "        if global_i % 1000 == 0:\n",
    "            print(f'global_i: {global_i}, epoch_i: {epoch_i}, loss: {loss.item()}')\n",
    "        \n",
    "        train_loss_hist.append((global_i, loss.item()))\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        global_i += 1\n",
    "        \n",
    "    # validation\n",
    "    model.eval()\n",
    "    valid_loss_list = []\n",
    "    \n",
    "    for batch in valid_dataloader:\n",
    "        x = torch.tensor(batch['doc_ids'])\n",
    "        y = torch.tensor(batch['label'])\n",
    "        \n",
    "        if use_cuda:\n",
    "            x = x.cuda()\n",
    "            y = y.cuda()\n",
    "            \n",
    "        y_pred = model(x)\n",
    "        loss = calc_loss(y_pred, y)\n",
    "        valid_loss_list.append(loss.item())\n",
    "        \n",
    "    valid_loss_mean = np.mean(valid_loss_list)\n",
    "    valid_loss_hist.append((global_i, valid_loss_mean.item()))\n",
    "    \n",
    "    if valid_loss_mean < min_valid_loss:\n",
    "        min_valid_loss = valid_loss_mean\n",
    "        best_epoch_i = epoch_i\n",
    "        best_model = deepcopy(model)\n",
    "        \n",
    "    if epoch_i % 1 == 0:\n",
    "        print('*'*30)\n",
    "        print(f'valid_loss_mean: {valid_loss_mean}')\n",
    "        print('*'*30)\n",
    "\n",
    "print(f'best_epoch_i: {best_epoch_i}, best_global_i: {global_i}')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
