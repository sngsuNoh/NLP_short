{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer, BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BERT_MODEL_NAME = 'bert-base-cased'\n",
    "\n",
    "bert_model = AutoModel.from_pretrained(BERT_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tokenizer = AutoTokenizer.from_pretrained(BERT_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28996\n"
     ]
    }
   ],
   "source": [
    "print(bert_tokenizer.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "·πÖ\n",
      "glowing\n",
      "surge\n",
      "deals\n",
      "recorded\n",
      "bye\n",
      "Cricket\n",
      "##alaya\n",
      "Beacon\n",
      "outs\n",
      "requirement\n",
      "##used\n"
     ]
    }
   ],
   "source": [
    "for i, key in enumerate(bert_tokenizer.get_vocab()):\n",
    "    print(key)\n",
    "    if i > 10: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.models.bert.tokenization_bert_fast.BertTokenizerFast"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(bert_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample1 = 'I am interested in data science'\n",
    "sample2 = 'Iaminterestedindatascience'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids: \n",
      "\ttensor([[ 101,  146, 1821, 3888, 1107, 2233, 2598,  102]])\n",
      "token_type_ids: \n",
      "\ttensor([[0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "attention_mask: \n",
      "\ttensor([[1, 1, 1, 1, 1, 1, 1, 1]])\n"
     ]
    }
   ],
   "source": [
    "tokenized_input_text = bert_tokenizer(sample1, return_tensors='pt')\n",
    "for key, value in tokenized_input_text.items():\n",
    "    print(f'{key}: \\n\\t{value}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids: \n",
      "\ttensor([[  101,   146, 11787, 22456, 24732, 20344, 10401, 25982,  3633,   102]])\n",
      "token_type_ids: \n",
      "\ttensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "attention_mask: \n",
      "\ttensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n"
     ]
    }
   ],
   "source": [
    "tokenized_input_text_merged = bert_tokenizer(sample2, return_tensors='pt')\n",
    "for key, value in tokenized_input_text_merged.items():\n",
    "    print(f'{key}: \\n\\t{value}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 101,  146, 1821, 3888, 1107, 2233, 2598,  102]])\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1]])\n"
     ]
    }
   ],
   "source": [
    "# print(tokenized_input_text['input_ids'])\n",
    "print(tokenized_input_text.input_ids)\n",
    "\n",
    "# print(tokenized_input_text['token_type_ids'])\n",
    "print(tokenized_input_text.token_type_ids)\n",
    "\n",
    "# print(tokenized_input_text['attention_mask'])\n",
    "print(tokenized_input_text.attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'am', 'interested', 'in', 'data', 'science']\n",
      "[101, 146, 1821, 3888, 1107, 2233, 2598, 102]\n",
      "[CLS] I am interested in data science [SEP]\n"
     ]
    }
   ],
   "source": [
    "tokenized_text = bert_tokenizer.tokenize(sample1)\n",
    "print(tokenized_text)\n",
    "\n",
    "input_ids = bert_tokenizer.encode(sample1)\n",
    "print(input_ids)\n",
    "\n",
    "decoded_ids = bert_tokenizer.decode(input_ids)\n",
    "print(decoded_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'am', 'interested', 'in', 'data', 'science']\n",
      "[146, 1821, 3888, 1107, 2233, 2598]\n",
      "I am interested in data science\n"
     ]
    }
   ],
   "source": [
    "tokenized_text = bert_tokenizer.tokenize(sample1, add_special_tokens=False)\n",
    "print(tokenized_text)\n",
    "\n",
    "input_ids = bert_tokenizer.encode(sample1, add_special_tokens=False)\n",
    "print(input_ids)\n",
    "\n",
    "decoded_ids = bert_tokenizer.decode(input_ids)\n",
    "print(decoded_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'am', 'interested', 'in', 'data']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_text = bert_tokenizer.tokenize(\n",
    "    sample1,\n",
    "    add_special_tokens=False,\n",
    "    max_length=5,\n",
    "    truncation=True\n",
    ")\n",
    "tokenized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([146, 1821, 3888, 1107, 2233], 'I am interested in data')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids = bert_tokenizer.encode(\n",
    "    sample1,\n",
    "    add_special_tokens=False,\n",
    "    max_length=5,\n",
    "    truncation=True\n",
    ")\n",
    "decoded_ids = bert_tokenizer.decode(input_ids)\n",
    "input_ids, decoded_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PAD]\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(bert_tokenizer.pad_token)\n",
    "print(bert_tokenizer.pad_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'am', 'interested', 'in', 'data', 'science', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "[146, 1821, 3888, 1107, 2233, 2598, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "I am interested in data science [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n"
     ]
    }
   ],
   "source": [
    "tokenized_text = bert_tokenizer.tokenize(\n",
    "    sample1,\n",
    "    add_special_tokens=False,\n",
    "    max_length=20,\n",
    "    padding='max_length'\n",
    ")\n",
    "print(tokenized_text)\n",
    "\n",
    "input_ids = bert_tokenizer.encode(\n",
    "    sample1,\n",
    "    add_special_tokens=False,\n",
    "    max_length=20,\n",
    "    padding='max_length'\n",
    ")\n",
    "print(input_ids)\n",
    "\n",
    "decoded_ids = bert_tokenizer.decode(input_ids)\n",
    "print(decoded_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ÌïúÍµ≠Ïñ¥ Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "kor_text = 'ÏïÑÎ≤ÑÏßÄ Í∞ÄÎ∞©Ïóê Îì§Ïñ¥Í∞ÄÏã†Îã§'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[UNK]', '[UNK]', '[UNK]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n"
     ]
    }
   ],
   "source": [
    "tokenized_text = bert_tokenizer.tokenize(\n",
    "    kor_text,\n",
    "    add_special_tokens=False,\n",
    "    max_length=20,\n",
    "    padding='max_length'\n",
    ")\n",
    "print(tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids: \n",
      "\ttensor([[101, 100, 100, 100, 102]])\n",
      "token_type_ids: \n",
      "\ttensor([[0, 0, 0, 0, 0]])\n",
      "attention_mask: \n",
      "\ttensor([[1, 1, 1, 1, 1]])\n"
     ]
    }
   ],
   "source": [
    "tokenized_input_text = bert_tokenizer(kor_text, return_tensors='pt')\n",
    "for key, value in tokenized_input_text.items():\n",
    "    print(f'{key}: \\n\\t{value}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[UNK]', '[UNK]', '[UNK]']\n",
      "[101, 100, 100, 100, 102]\n",
      "[CLS] [UNK] [UNK] [UNK] [SEP]\n"
     ]
    }
   ],
   "source": [
    "tokenized_text = bert_tokenizer.tokenize(kor_text)\n",
    "print(tokenized_text)\n",
    "\n",
    "input_ids = bert_tokenizer.encode(kor_text)\n",
    "print(input_ids)\n",
    "\n",
    "decoded_ids = bert_tokenizer.decode(input_ids)\n",
    "print(decoded_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "MULTI_BERT_MODEL_NAME = 'bert-base-multilingual-cased'\n",
    "\n",
    "multi_bert_model = AutoModel.from_pretrained(MULTI_BERT_MODEL_NAME)\n",
    "multi_bert_tokenizer = AutoTokenizer.from_pretrained(MULTI_BERT_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "unk_text = 'ÌïúÍæ∫Ïù∏ Îú∞Îßå ÏïåÏïÑÎΩà Ïë§ ÏûàÍºê ÏßûÏçΩÌïòÍº§ÏîÅÎãàÎî∞'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[UNK]', '[UNK]', '[UNK]', 'Ïë§', '[UNK]', '[UNK]']\n",
      "[100, 100, 100, 9510, 100, 100]\n",
      "[UNK] [UNK] [UNK] Ïë§ [UNK] [UNK]\n"
     ]
    }
   ],
   "source": [
    "tokenized_text = multi_bert_tokenizer.tokenize(unk_text, add_special_tokens=False)\n",
    "print(tokenized_text)\n",
    "\n",
    "input_ids = multi_bert_tokenizer.encode(unk_text, add_special_tokens=False)\n",
    "print(input_ids)\n",
    "\n",
    "decoded_ids = multi_bert_tokenizer.decode(input_ids)\n",
    "print(decoded_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "['ÌïúÍæ∫Ïù∏', 'Îú∞Îßå', 'ÏïåÏïÑÎΩà', 'Ïë§', 'ÏûàÍºê', 'ÏßûÏçΩÌïòÍº§ÏîÅÎãàÎî∞']\n",
      "[119547, 119548, 119549, 9510, 119550, 119551]\n",
      "ÌïúÍæ∫Ïù∏ Îú∞Îßå ÏïåÏïÑÎΩà Ïë§ ÏûàÍºê ÏßûÏçΩÌïòÍº§ÏîÅÎãàÎî∞\n"
     ]
    }
   ],
   "source": [
    "added_token_num = multi_bert_tokenizer.add_tokens(['ÌïúÍæ∫Ïù∏', 'Îú∞Îßå', 'ÏïåÏïÑÎΩà', 'ÏûàÍºê', 'ÏßûÏçΩÌïòÍº§ÏîÅÎãàÎî∞'])\n",
    "print(added_token_num)\n",
    "\n",
    "tokenized_text = multi_bert_tokenizer.tokenize(unk_text, add_special_tokens=False)\n",
    "print(tokenized_text)\n",
    "\n",
    "input_ids = multi_bert_tokenizer.encode(unk_text, add_special_tokens=False)\n",
    "print(input_ids)\n",
    "\n",
    "decoded_ids = multi_bert_tokenizer.decode(input_ids)\n",
    "print(decoded_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[', 'DA', '##D', ']', 'ÏïÑ', '##Îπ†', '[', '/', 'DA', '##D', ']', 'Í∞Ä', 'Î∞©', '##Ïóê', 'Îì§Ïñ¥', '##Í∞Ä', '##Ïã†', '##Îã§']\n",
      "[164, 47855, 11490, 166, 9519, 119008, 164, 120, 47855, 11490, 166, 8843, 9328, 10530, 71568, 11287, 25387, 11903]\n",
      "[ DAD ] ÏïÑÎπ† [ / DAD ] Í∞Ä Î∞©Ïóê Îì§Ïñ¥Í∞ÄÏã†Îã§\n"
     ]
    }
   ],
   "source": [
    "special_token_text = '[DAD]ÏïÑÎπ†[/DAD]Í∞Ä Î∞©Ïóê Îì§Ïñ¥Í∞ÄÏã†Îã§'\n",
    "\n",
    "tokenized_text = multi_bert_tokenizer.tokenize(special_token_text, add_special_tokens=False)\n",
    "print(tokenized_text)\n",
    "\n",
    "input_ids = multi_bert_tokenizer.encode(special_token_text, add_special_tokens=False)\n",
    "print(input_ids)\n",
    "\n",
    "decoded_ids = multi_bert_tokenizer.decode(input_ids)\n",
    "print(decoded_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[DAD]', 'ÏïÑ', '##Îπ†', '[/DAD]', 'Í∞Ä', 'Î∞©', '##Ïóê', 'Îì§Ïñ¥', '##Í∞Ä', '##Ïã†', '##Îã§']\n",
      "[119552, 9519, 119008, 119553, 8843, 9328, 10530, 71568, 11287, 25387, 11903]\n",
      "[DAD] ÏïÑÎπ† [/DAD] Í∞Ä Î∞©Ïóê Îì§Ïñ¥Í∞ÄÏã†Îã§\n"
     ]
    }
   ],
   "source": [
    "special_token_text = '[DAD]ÏïÑÎπ†[/DAD]Í∞Ä Î∞©Ïóê Îì§Ïñ¥Í∞ÄÏã†Îã§'\n",
    "\n",
    "added_token_num = multi_bert_tokenizer.add_special_tokens({'additional_special_tokens': ['[DAD]', '[/DAD]']})\n",
    "\n",
    "tokenized_text = multi_bert_tokenizer.tokenize(special_token_text, add_special_tokens=False)\n",
    "print(tokenized_text)\n",
    "\n",
    "input_ids = multi_bert_tokenizer.encode(special_token_text, add_special_tokens=False)\n",
    "print(input_ids)\n",
    "\n",
    "decoded_ids = multi_bert_tokenizer.decode(input_ids)\n",
    "print(decoded_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ÏïÑÎπ† Í∞Ä Î∞©Ïóê Îì§Ïñ¥Í∞ÄÏã†Îã§\n"
     ]
    }
   ],
   "source": [
    "decoded_ids = multi_bert_tokenizer.decode(input_ids, skip_special_tokens=True)\n",
    "print(decoded_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens (int):       [101, 9519, 119008, 11287, 9328, 10530, 71568, 11287, 25387, 11903, 102, 0, 0]\n",
      "Tokens (str):       [['[CLS]', 'ÏïÑ', '##Îπ†', '##Í∞Ä', 'Î∞©', '##Ïóê', 'Îì§Ïñ¥', '##Í∞Ä', '##Ïã†', '##Îã§', '[SEP]', '[PAD]', '[PAD]'], ['[CLS]', '[DAD]', 'ÏïÑ', '##Îπ†', '[/DAD]', 'Í∞Ä', '##Î∞©', '##Ïóê', '##Îì§Ïñ¥', '##Í∞Ä', '##Ïã†', '##Îã§', '[SEP]']]\n",
      "Tokens (attn_mask): [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0]\n",
      "\n",
      "Tokens (int):       [101, 119552, 9519, 119008, 119553, 8843, 42337, 10530, 93200, 11287, 25387, 11903, 102]\n",
      "Tokens (str):       [['[CLS]', 'ÏïÑ', '##Îπ†', '##Í∞Ä', 'Î∞©', '##Ïóê', 'Îì§Ïñ¥', '##Í∞Ä', '##Ïã†', '##Îã§', '[SEP]', '[PAD]', '[PAD]'], ['[CLS]', '[DAD]', 'ÏïÑ', '##Îπ†', '[/DAD]', 'Í∞Ä', '##Î∞©', '##Ïóê', '##Îì§Ïñ¥', '##Í∞Ä', '##Ïã†', '##Îã§', '[SEP]']]\n",
      "Tokens (attn_mask): [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample_list = [\n",
    "    'ÏïÑÎπ†Í∞Ä Î∞©Ïóê Îì§Ïñ¥Í∞ÄÏã†Îã§',\n",
    "    '[DAD]ÏïÑÎπ†[/DAD]Í∞ÄÎ∞©ÏóêÎì§Ïñ¥Í∞ÄÏã†Îã§'\n",
    "]\n",
    "\n",
    "tokens = multi_bert_tokenizer(\n",
    "    sample_list,\n",
    "    padding=True\n",
    ")\n",
    "\n",
    "for i in range(2):\n",
    "    print(f'Tokens (int):       {tokens['input_ids'][i]}')\n",
    "    print(f'Tokens (str):       {[multi_bert_tokenizer.convert_ids_to_tokens(s) for s in tokens['input_ids']]}')\n",
    "    print(f'Tokens (attn_mask): {tokens['attention_mask'][i]}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [MASK] ÌÜ†ÌÅ∞ ÏòàÏ∏°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ÏïÑ', '##Îπ†', '##Í∞Ä', '[MASK]', 'Îì§Ïñ¥', '##Í∞Ä', '##Ïã†', '##Îã§']\n"
     ]
    }
   ],
   "source": [
    "masked_text = 'ÏïÑÎπ†Í∞Ä [MASK] Îì§Ïñ¥Í∞ÄÏã†Îã§'\n",
    "tokenized_text = multi_bert_tokenizer.tokenize(masked_text)\n",
    "\n",
    "print(tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertForMaskedLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From üëâv4.50üëà onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'score': 0.05300595983862877,\n",
       "  'token': 9654,\n",
       "  'token_str': 'Ïûò',\n",
       "  'sequence': 'ÏïÑÎπ†Í∞Ä Ïûò Îì§Ïñ¥Í∞ÄÏã†Îã§'},\n",
       " {'score': 0.05062916874885559,\n",
       "  'token': 11287,\n",
       "  'token_str': '##Í∞Ä',\n",
       "  'sequence': 'ÏïÑÎπ†Í∞ÄÍ∞Ä Îì§Ïñ¥Í∞ÄÏã†Îã§'},\n",
       " {'score': 0.047388385981321335,\n",
       "  'token': 8982,\n",
       "  'token_str': 'ÎÇò',\n",
       "  'sequence': 'ÏïÑÎπ†Í∞Ä ÎÇò Îì§Ïñ¥Í∞ÄÏã†Îã§'},\n",
       " {'score': 0.03328689560294151,\n",
       "  'token': 9056,\n",
       "  'token_str': 'Îã§',\n",
       "  'sequence': 'ÏïÑÎπ†Í∞Ä Îã§ Îì§Ïñ¥Í∞ÄÏã†Îã§'},\n",
       " {'score': 0.026803359389305115,\n",
       "  'token': 14867,\n",
       "  'token_str': '##Î©¥',\n",
       "  'sequence': 'ÏïÑÎπ†Í∞ÄÎ©¥ Îì§Ïñ¥Í∞ÄÏã†Îã§'}]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "nlp_fill = pipeline('fill-mask', model=MULTI_BERT_MODEL_NAME)\n",
    "nlp_fill(masked_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### ÌïúÍµ≠Ïñ¥ Ï†ÑÏö© Î™®Îç∏Ïù¥ ÏïÑÎãå multi-lingual BERTÎ•º ÌôúÏö©ÌñàÎçîÎãà ÏïÑÏâ¨Ïö¥ Í≤∞Í≥ºÍ∞Ä ÎÇòÌÉÄÎÇúÎã§ .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ÏòÅÏñ¥ Î¨∏Ïû•ÏóêÏÑú [MASK] ÌÜ†ÌÅ∞ ÏòàÏ∏°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Leave', 'before', 'you', '[MASK]', 'me']\n"
     ]
    }
   ],
   "source": [
    "masked_text = 'Leave before you [MASK] me'\n",
    "tokenized_text = multi_bert_tokenizer.tokenize(masked_text)\n",
    "\n",
    "print(tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'score': 0.3139025568962097,\n",
       "  'token': 11248,\n",
       "  'token_str': 'Love',\n",
       "  'sequence': 'Leave before you Love me'},\n",
       " {'score': 0.04809042811393738,\n",
       "  'token': 16138,\n",
       "  'token_str': 'love',\n",
       "  'sequence': 'Leave before you love me'},\n",
       " {'score': 0.04090191423892975,\n",
       "  'token': 10169,\n",
       "  'token_str': 'with',\n",
       "  'sequence': 'Leave before you with me'},\n",
       " {'score': 0.035010598599910736,\n",
       "  'token': 99401,\n",
       "  'token_str': 'Loved',\n",
       "  'sequence': 'Leave before you Loved me'},\n",
       " {'score': 0.030007442459464073,\n",
       "  'token': 10135,\n",
       "  'token_str': 'on',\n",
       "  'sequence': 'Leave before you on me'}]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "nlp_fill = pipeline('fill-mask', model=MULTI_BERT_MODEL_NAME)\n",
    "nlp_fill(masked_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids:\n",
      "\ttensor([[   101,   9519, 119008,  11287,   9328,  10530,  71568,  11287,  25387,\n",
      "          11903,    102]])\n",
      "token_type_ids:\n",
      "\ttensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "attention_mask:\n",
      "\ttensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "\n",
      "Token wise output: torch.Size([1, 11, 768]), Pooled output: torch.Size([1, 768])\n"
     ]
    }
   ],
   "source": [
    "tokens_pt = multi_bert_tokenizer('ÏïÑÎπ†Í∞Ä Î∞©Ïóê Îì§Ïñ¥Í∞ÄÏã†Îã§', return_tensors='pt')\n",
    "for key, value in tokens_pt.items():\n",
    "    print(f'{key}:\\n\\t{value}')\n",
    "    \n",
    "outputs = multi_bert_model(**tokens_pt)\n",
    "last_hidden_state = outputs.last_hidden_state\n",
    "pooler_output = outputs.pooler_output\n",
    "\n",
    "print(f'\\nToken wise output: {last_hidden_state.shape}, Pooled output: {pooler_output.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.9587e-01, -2.6240e-03,  2.1747e-01, -7.7761e-02, -1.7742e-03,\n",
      "          4.5689e-01,  5.9940e-02,  2.5821e-01, -3.6000e-01,  2.5447e-01,\n",
      "          1.8904e-02, -1.8748e-01, -1.4784e-01, -1.1042e-01,  1.4066e-01,\n",
      "         -1.7055e-01,  4.9949e-01,  4.0194e-02, -6.1990e-02, -3.2384e-01,\n",
      "         -9.9998e-01, -1.0523e-01, -1.9167e-01, -5.8405e-02, -2.5229e-01,\n",
      "         -1.8232e-02, -1.0872e-01, -4.8203e-02,  7.6683e-02, -1.2361e-01,\n",
      "         -7.1717e-02, -9.9999e-01,  3.4002e-01,  4.9053e-01,  1.4669e-01,\n",
      "         -1.1793e-02,  5.8997e-02,  2.0018e-01,  1.8910e-01, -3.4949e-01,\n",
      "         -1.8250e-01,  3.9466e-03, -9.3936e-02,  1.9231e-01, -1.2782e-01,\n",
      "         -2.0957e-01, -1.0196e-01,  1.7126e-01, -2.4489e-01,  9.5668e-02,\n",
      "          1.2766e-01,  1.9946e-01,  3.0436e-01,  2.3804e-01,  1.3247e-01,\n",
      "          1.6288e-01,  1.1462e-01,  1.5527e-01,  2.3521e-01, -2.1541e-03,\n",
      "         -2.7164e-02,  1.0165e-01, -2.4749e-02, -1.1822e-01, -1.8863e-01,\n",
      "         -3.7550e-01, -9.1158e-02, -1.8992e-02,  4.8320e-01, -2.1429e-02,\n",
      "         -1.9781e-01, -3.1172e-01, -1.4527e-01,  7.5715e-02,  1.1050e-03,\n",
      "         -1.2616e-01,  2.2306e-01,  2.6539e-01, -8.3114e-02,  1.6549e-02,\n",
      "         -2.0332e-01, -3.4146e-01, -1.7324e-01,  1.3737e-01, -1.4453e-01,\n",
      "          2.6747e-01,  1.4524e-01, -1.6389e-01,  1.2013e-02,  8.6950e-02,\n",
      "          2.2246e-01,  4.0476e-01, -3.0587e-01,  2.9608e-01, -1.0588e-01,\n",
      "         -1.4684e-01, -6.3002e-01, -4.9289e-02, -2.8388e-02, -2.9376e-01,\n",
      "         -7.6358e-02,  7.5313e-02, -2.2673e-01, -1.4664e-01, -1.6943e-01,\n",
      "         -1.5476e-01,  7.8815e-02,  1.9294e-01, -9.6744e-02,  2.5195e-01,\n",
      "         -1.2892e-01, -2.3805e-01, -1.2216e-02, -7.9019e-03, -7.1010e-02,\n",
      "          8.8535e-01, -1.0916e-01,  1.0827e-01, -3.0459e-01, -1.5857e-01,\n",
      "         -4.8748e-01,  9.9998e-01,  3.1884e-02, -1.5570e-01,  5.1824e-02,\n",
      "          7.9979e-02, -4.7865e-01,  1.4429e-01,  1.8522e-01,  1.8042e-01,\n",
      "          2.0705e-02, -1.9244e-02, -4.4509e-02, -2.9725e-01, -5.9554e-01,\n",
      "         -1.3196e-01, -1.9210e-01,  2.6356e-01, -2.5723e-01, -1.9087e-01,\n",
      "          1.2952e-01,  4.8163e-01,  1.2234e-01, -2.5590e-02, -1.1944e-01,\n",
      "         -6.2398e-02,  2.0186e-01, -1.8525e-01,  9.9998e-01,  5.2773e-01,\n",
      "          8.2870e-02, -1.4188e-01,  2.4555e-01, -5.6447e-01, -3.4784e-01,\n",
      "         -1.6837e-01, -2.7886e-01, -4.2496e-01,  2.5460e-01,  1.1097e-01,\n",
      "         -7.1423e-02, -1.2547e-01, -1.4676e-01, -1.2166e-01,  2.0880e-01,\n",
      "         -5.3468e-01, -1.4802e-01,  3.2102e-01,  2.8113e-01,  2.5320e-01,\n",
      "          1.8814e-02,  2.8161e-01,  4.1996e-02, -1.6826e-01, -4.5177e-02,\n",
      "          1.5953e-01,  1.0114e-01, -8.5969e-02, -3.2224e-03,  9.9679e-03,\n",
      "          2.2792e-02, -3.5153e-02, -2.5704e-01,  6.9191e-03, -1.4643e-01,\n",
      "         -4.4352e-01,  2.1875e-02, -6.7388e-03, -5.5773e-02,  1.5387e-01,\n",
      "         -5.4894e-02,  1.2261e-01, -1.8776e-01,  1.0767e-01,  2.4482e-01,\n",
      "          5.8599e-02, -3.4786e-01,  2.4759e-01,  1.8563e-01,  8.7917e-02,\n",
      "          7.4140e-02, -5.4580e-04,  1.1590e-02,  9.2006e-02, -5.2249e-02,\n",
      "         -3.3527e-01,  1.3781e-01,  7.1211e-02,  1.9949e-01, -1.0932e-01,\n",
      "         -3.6033e-01, -1.5118e-01,  5.2082e-01,  2.5986e-01, -2.1740e-01,\n",
      "          1.0955e-01,  2.2222e-01, -9.1015e-02, -8.5304e-03,  1.4717e-01,\n",
      "         -1.7063e-01, -2.1931e-01, -2.8794e-01, -6.0252e-02,  1.3968e-02,\n",
      "          1.8516e-01,  1.3741e-01,  2.2894e-01,  4.1930e-02, -1.5801e-01,\n",
      "         -3.0714e-02, -6.0022e-02,  2.1827e-02,  2.6737e-01,  2.1056e-02,\n",
      "          6.2072e-01, -1.7357e-01,  1.8256e-01, -3.8184e-01, -1.2861e-01,\n",
      "          2.3974e-01, -2.6045e-01,  1.8239e-01,  8.6396e-01, -3.1072e-02,\n",
      "         -1.1410e-01,  2.1653e-01,  1.8559e-01,  2.1691e-02, -1.3638e-01,\n",
      "         -1.1073e-01, -5.3561e-01,  4.3314e-01,  1.1318e-01,  1.3472e-01,\n",
      "         -9.9998e-01,  1.8723e-01,  8.5478e-02,  3.2882e-01,  1.2699e-01,\n",
      "          1.7724e-01, -4.8996e-02,  1.9568e-01,  7.8120e-01, -2.9788e-01,\n",
      "         -3.1636e-01, -1.8120e-01, -1.6184e-01, -4.0299e-01, -1.4006e-01,\n",
      "         -6.1229e-02, -2.4455e-01,  2.5620e-02,  5.0919e-03, -3.7989e-02,\n",
      "          1.7100e-01,  2.3855e-01, -9.4709e-01,  6.8552e-01, -8.4643e-03,\n",
      "          2.0836e-02, -8.2448e-02,  2.2187e-01, -9.9998e-01,  1.5947e-01,\n",
      "         -7.5055e-02, -1.7205e-01,  1.5849e-01, -4.1480e-01, -2.1217e-01,\n",
      "          1.6907e-01,  3.3336e-01,  2.5240e-01,  1.5066e-01,  1.6760e-01,\n",
      "          4.3530e-01, -8.9512e-02,  8.6444e-02,  1.7018e-01, -1.6372e-01,\n",
      "          4.7351e-01,  1.9271e-02,  5.6935e-02,  2.9571e-01, -3.1401e-02,\n",
      "          2.1309e-01, -1.0960e-01,  2.3319e-01,  2.9276e-01,  3.1736e-02,\n",
      "         -1.0316e-02, -1.0720e-01,  1.5483e-01, -6.4426e-01, -2.5115e-03,\n",
      "         -2.7347e-01,  3.7688e-02, -9.2881e-02,  3.3822e-03, -7.0008e-02,\n",
      "         -2.4711e-01,  6.4624e-02, -6.8711e-02,  9.9999e-01,  8.0641e-02,\n",
      "         -6.0083e-02, -2.1243e-01,  4.5557e-01,  3.5471e-01, -1.8537e-01,\n",
      "         -2.8515e-01, -7.4099e-02,  3.8037e-01,  2.8503e-01,  1.5397e-01,\n",
      "          6.0991e-02, -1.7710e-01,  2.6666e-01, -1.4237e-01, -1.8893e-01,\n",
      "          1.0865e-01, -3.0960e-01,  1.3755e-01, -3.1823e-02, -1.6796e-01,\n",
      "          1.3906e-01, -8.4458e-02, -1.2528e-01, -5.4610e-01,  1.0271e-01,\n",
      "          1.9306e-02, -7.5657e-02,  9.5575e-02,  1.3101e-01, -1.6367e-01,\n",
      "          3.8682e-01,  2.7129e-01, -7.2034e-02, -2.6243e-01, -1.4642e-01,\n",
      "         -1.2506e-01, -1.3002e-02, -1.5354e-01, -1.8716e-01,  9.7031e-02,\n",
      "         -5.6943e-01,  7.8196e-02,  7.0775e-02, -7.7185e-02, -1.8023e-01,\n",
      "          1.1501e-01, -9.9999e-01, -1.4032e-01,  1.4356e-01, -1.8590e-01,\n",
      "          1.8714e-01, -2.4041e-01, -7.8657e-02,  2.8563e-01,  9.2214e-02,\n",
      "          9.0260e-02, -1.7355e-02, -2.4300e-01, -7.9134e-03, -6.9019e-02,\n",
      "          6.9532e-02,  6.8351e-01,  4.3359e-01,  7.7587e-02, -2.4850e-01,\n",
      "          3.7973e-02, -5.3665e-01, -2.2849e-01,  1.2429e-01,  1.1936e-01,\n",
      "         -1.4512e-01,  1.3273e-01,  1.4424e-01,  9.0693e-02, -4.0045e-04,\n",
      "          2.3390e-01,  4.8001e-02, -7.1796e-02,  1.2645e-01,  4.4172e-02,\n",
      "         -9.2496e-02, -3.5659e-02,  2.4765e-01, -2.4248e-01,  2.3167e-01,\n",
      "          9.9585e-02,  1.7646e-01,  9.6674e-02,  2.4197e-01, -1.5496e-01,\n",
      "          1.0628e-02, -6.4846e-02, -2.0940e-02, -1.4697e-01, -1.3496e-01,\n",
      "         -4.5919e-02,  9.9999e-01,  2.4503e-01,  2.5589e-01, -2.2397e-01,\n",
      "          2.1227e-01,  2.2888e-01, -2.7171e-01,  1.3348e-01,  1.3002e-01,\n",
      "          1.1882e-01, -8.2760e-02, -5.9564e-02, -1.2988e-02,  2.5891e-01,\n",
      "          2.5981e-01,  1.1929e-01,  3.2453e-01, -1.9974e-01,  5.9337e-01,\n",
      "         -1.1113e-01, -2.0308e-01, -9.6261e-01,  1.2488e-01,  1.5999e-01,\n",
      "         -3.2971e-01, -2.3628e-01,  9.4170e-02, -1.4048e-01,  5.2453e-04,\n",
      "         -1.1811e-01,  1.1951e-01,  1.3372e-01, -5.5887e-02,  3.1142e-01,\n",
      "         -2.3672e-01,  9.9998e-01, -1.8549e-02,  1.4224e-01,  3.2876e-01,\n",
      "          2.0036e-01, -5.8975e-02, -4.4691e-02,  7.4655e-03,  1.4532e-01,\n",
      "          3.0970e-02,  1.4261e-01, -8.2311e-01,  2.0878e-01,  9.5343e-02,\n",
      "          3.3448e-01, -1.1193e-01,  2.2900e-01, -3.3547e-01,  2.3534e-01,\n",
      "          4.7436e-02, -1.3236e-01, -1.5534e-01,  1.2797e-01, -2.6306e-01,\n",
      "          3.8268e-01, -9.8025e-02,  1.5060e-01, -2.0218e-01,  1.3339e-01,\n",
      "         -1.4757e-02,  2.7518e-01, -2.8087e-02,  6.4298e-02, -7.5405e-02,\n",
      "         -2.0170e-01, -1.2869e-01,  8.3827e-02, -3.4589e-01,  9.9998e-01,\n",
      "          1.3261e-02,  1.8180e-03, -8.5007e-02,  9.0254e-02, -1.4663e-01,\n",
      "          1.5951e-01,  5.5024e-01, -2.5219e-01,  2.2425e-01,  2.5064e-01,\n",
      "         -5.2638e-01,  1.0966e-01,  3.1039e-02, -3.7111e-01, -1.7898e-01,\n",
      "          8.5936e-01, -1.1702e-01,  3.4544e-01,  2.4337e-01,  1.3548e-01,\n",
      "         -6.5729e-02, -1.5993e-01,  1.4500e-01,  7.1231e-01,  1.3691e-01,\n",
      "          1.0695e-01,  8.5950e-02,  3.1181e-02, -2.7397e-01, -1.7588e-01,\n",
      "          9.9998e-01,  9.9998e-01, -1.4026e-01,  2.9632e-01, -3.3513e-01,\n",
      "         -1.1838e-01, -9.9836e-02,  1.5859e-01,  2.0804e-01,  6.4897e-02,\n",
      "         -6.6168e-02, -2.5406e-02, -2.6075e-01, -1.9851e-01, -1.2550e-01,\n",
      "          9.3052e-02, -2.0985e-02,  1.5884e-02, -1.5282e-01,  4.2351e-01,\n",
      "          2.6995e-01, -5.7979e-02,  3.9035e-01,  1.1630e-01,  8.7117e-02,\n",
      "         -7.6105e-02, -1.2758e-01,  4.3324e-01,  5.1462e-02,  8.4135e-02,\n",
      "         -3.2104e-01, -7.1986e-03, -9.9998e-01, -2.2418e-02, -1.1426e-01,\n",
      "         -1.3862e-01,  4.5055e-01,  1.1094e-01, -3.0327e-02, -1.9225e-01,\n",
      "          2.2947e-02, -2.7670e-01,  9.1053e-02,  1.3299e-01, -1.2294e-01,\n",
      "         -9.5499e-02, -3.0016e-01,  3.1042e-01, -3.2855e-01,  8.0116e-02,\n",
      "         -2.1817e-01, -1.3449e-01, -5.0797e-01, -1.7080e-01, -1.9487e-01,\n",
      "          2.5207e-01, -7.1846e-02, -1.8411e-03,  1.6415e-01,  1.9837e-01,\n",
      "          1.6479e-01, -1.8079e-01,  2.9474e-01, -2.6560e-01,  1.2053e-01,\n",
      "          6.9674e-02,  1.5513e-01,  3.0362e-02, -1.2606e-01, -1.7946e-01,\n",
      "         -1.1620e-01, -1.0518e-01, -1.2159e-01,  1.5965e-01, -1.5149e-01,\n",
      "          1.6010e-01, -1.4408e-01,  1.2217e-01, -1.2560e-01,  1.0746e-01,\n",
      "          1.7880e-01,  3.3006e-01, -2.2736e-01,  2.9684e-01,  1.5087e-01,\n",
      "         -3.2634e-02,  4.0786e-01, -4.6434e-02, -2.7211e-02, -1.1294e-01,\n",
      "          9.9999e-01,  3.0628e-01,  8.2382e-02,  8.3876e-02,  5.5095e-02,\n",
      "          2.7523e-01,  1.4371e-01,  3.1079e-01, -1.4480e-02,  4.2676e-01,\n",
      "         -1.3497e-01,  5.1629e-02,  1.3431e-01,  1.1831e-01,  3.3123e-02,\n",
      "          5.2938e-02,  2.4852e-01,  4.9797e-01,  9.0710e-02,  2.2048e-01,\n",
      "          1.9014e-01,  8.3382e-02,  1.7075e-01,  2.6972e-01,  2.5281e-01,\n",
      "          2.0093e-01,  1.2763e-01, -2.0838e-01,  1.8865e-01, -3.0578e-02,\n",
      "         -1.3587e-01,  1.1296e-01, -1.3863e-01, -8.1828e-02,  6.2068e-02,\n",
      "         -1.5972e-01, -3.6474e-02, -1.4922e-02,  1.5289e-01, -1.1513e-01,\n",
      "          2.7655e-01, -6.3435e-02, -1.1446e-01,  4.1785e-01, -3.8539e-01,\n",
      "          1.8629e-01, -1.4900e-01,  3.6604e-02, -6.6993e-01,  1.0879e-01,\n",
      "         -5.9363e-02, -3.6942e-01, -5.3368e-02, -2.4822e-01,  1.0729e-01,\n",
      "          1.2749e-01, -5.0256e-02,  6.8922e-02, -1.7184e-01,  2.0374e-01,\n",
      "         -8.2826e-02, -8.2621e-02, -4.0181e-02, -9.9998e-01,  1.5947e-01,\n",
      "          2.4592e-02, -1.7221e-01,  4.1944e-02,  3.2077e-02,  1.8045e-01,\n",
      "          1.4097e-01, -1.1131e-01, -4.4525e-02, -4.5816e-03,  1.9963e-01,\n",
      "         -1.8684e-01, -8.7225e-02,  1.7194e-01, -3.5694e-01, -1.8910e-01,\n",
      "         -9.1483e-02,  2.7386e-03,  1.5259e-01,  2.7543e-01, -2.4543e-01,\n",
      "          1.2021e-01, -2.0454e-01,  1.7886e-01,  7.5572e-02,  2.8039e-01,\n",
      "         -2.0991e-01, -9.7650e-02,  1.8656e-01, -3.5086e-01, -2.3330e-01,\n",
      "         -1.6021e-01,  6.3154e-02, -5.8566e-02,  4.2302e-02,  2.9784e-02,\n",
      "         -1.3356e-02,  2.5208e-01, -1.1997e-01,  2.8404e-01, -1.7913e-01,\n",
      "          2.6091e-01, -6.8252e-01, -2.0010e-01, -2.8045e-01, -8.5005e-02,\n",
      "          2.7374e-01,  1.9878e-01,  2.5006e-02,  9.4972e-02,  1.2602e-02,\n",
      "          1.1115e-01,  2.1789e-02,  6.5068e-02,  1.2082e-01, -2.3944e-02,\n",
      "         -5.5142e-05, -6.3549e-02,  2.9806e-01, -2.3229e-01,  3.6442e-02,\n",
      "         -9.4620e-01, -6.6874e-02,  8.4687e-02,  2.4827e-01,  2.8518e-01,\n",
      "         -1.5375e-01, -3.1697e-02, -2.3436e-01,  9.1477e-02,  1.2307e-01,\n",
      "          2.0571e-01,  1.9277e-01, -8.4275e-03,  1.5696e-01, -2.1149e-01,\n",
      "          2.0879e-03,  4.8534e-01, -2.2887e-01, -2.0023e-02,  2.6604e-01,\n",
      "          8.5707e-02,  4.8420e-01,  2.0800e-01,  3.2389e-01,  1.7072e-01,\n",
      "         -3.1909e-01,  1.7457e-01,  8.4647e-02]], grad_fn=<TanhBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(pooler_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding(119547, 768, padding_idx=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding(119549, 768, padding_idx=0)\n"
     ]
    }
   ],
   "source": [
    "print(multi_bert_model.get_input_embeddings())\n",
    "multi_bert_model.resize_token_embeddings(multi_bert_tokenizer.vocab_size + added_token_num)\n",
    "print(multi_bert_model.get_input_embeddings())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
