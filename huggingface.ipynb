{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer, BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "BERT_MODEL_NAME = 'bert-base-cased'\n",
    "\n",
    "bert_model = AutoModel.from_pretrained(BERT_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tokenizer = AutoTokenizer.from_pretrained(BERT_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28996\n"
     ]
    }
   ],
   "source": [
    "print(bert_tokenizer.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##arded\n",
      "##ations\n",
      "controlled\n",
      "exceptions\n",
      "##zzy\n",
      "headlines\n",
      "Carr\n",
      "##sko\n",
      "Vita\n",
      "paused\n",
      "âˆš\n",
      "2019\n"
     ]
    }
   ],
   "source": [
    "for i, key in enumerate(bert_tokenizer.get_vocab()):\n",
    "    print(key)\n",
    "    if i > 10: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.models.bert.tokenization_bert_fast.BertTokenizerFast"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(bert_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample1 = 'I am interested in data science'\n",
    "sample2 = 'Iaminterestedindatascience'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids: \n",
      "\ttensor([[ 101,  146, 1821, 3888, 1107, 2233, 2598,  102]])\n",
      "token_type_ids: \n",
      "\ttensor([[0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "attention_mask: \n",
      "\ttensor([[1, 1, 1, 1, 1, 1, 1, 1]])\n"
     ]
    }
   ],
   "source": [
    "tokenized_input_text = bert_tokenizer(sample1, return_tensors='pt')\n",
    "for key, value in tokenized_input_text.items():\n",
    "    print(f'{key}: \\n\\t{value}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids: \n",
      "\ttensor([[  101,   146, 11787, 22456, 24732, 20344, 10401, 25982,  3633,   102]])\n",
      "token_type_ids: \n",
      "\ttensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "attention_mask: \n",
      "\ttensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n"
     ]
    }
   ],
   "source": [
    "tokenized_input_text_merged = bert_tokenizer(sample2, return_tensors='pt')\n",
    "for key, value in tokenized_input_text_merged.items():\n",
    "    print(f'{key}: \\n\\t{value}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 101,  146, 1821, 3888, 1107, 2233, 2598,  102]])\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1]])\n"
     ]
    }
   ],
   "source": [
    "# print(tokenized_input_text['input_ids'])\n",
    "print(tokenized_input_text.input_ids)\n",
    "\n",
    "# print(tokenized_input_text['token_type_ids'])\n",
    "print(tokenized_input_text.token_type_ids)\n",
    "\n",
    "# print(tokenized_input_text['attention_mask'])\n",
    "print(tokenized_input_text.attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'am', 'interested', 'in', 'data', 'science']\n",
      "[101, 146, 1821, 3888, 1107, 2233, 2598, 102]\n",
      "[CLS] I am interested in data science [SEP]\n"
     ]
    }
   ],
   "source": [
    "tokenized_text = bert_tokenizer.tokenize(sample1)\n",
    "print(tokenized_text)\n",
    "\n",
    "input_ids = bert_tokenizer.encode(sample1)\n",
    "print(input_ids)\n",
    "\n",
    "decoded_ids = bert_tokenizer.decode(input_ids)\n",
    "print(decoded_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'am', 'interested', 'in', 'data', 'science']\n",
      "[146, 1821, 3888, 1107, 2233, 2598]\n",
      "I am interested in data science\n"
     ]
    }
   ],
   "source": [
    "tokenized_text = bert_tokenizer.tokenize(sample1, add_special_tokens=False)\n",
    "print(tokenized_text)\n",
    "\n",
    "input_ids = bert_tokenizer.encode(sample1, add_special_tokens=False)\n",
    "print(input_ids)\n",
    "\n",
    "decoded_ids = bert_tokenizer.decode(input_ids)\n",
    "print(decoded_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'am', 'interested', 'in', 'data']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_text = bert_tokenizer.tokenize(\n",
    "    sample1,\n",
    "    add_special_tokens=False,\n",
    "    max_length=5,\n",
    "    truncation=True\n",
    ")\n",
    "tokenized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([146, 1821, 3888, 1107, 2233], 'I am interested in data')"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids = bert_tokenizer.encode(\n",
    "    sample1,\n",
    "    add_special_tokens=False,\n",
    "    max_length=5,\n",
    "    truncation=True\n",
    ")\n",
    "decoded_ids = bert_tokenizer.decode(input_ids)\n",
    "input_ids, decoded_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PAD]\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(bert_tokenizer.pad_token)\n",
    "print(bert_tokenizer.pad_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'am', 'interested', 'in', 'data', 'science', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "[146, 1821, 3888, 1107, 2233, 2598, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "I am interested in data science [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n"
     ]
    }
   ],
   "source": [
    "tokenized_text = bert_tokenizer.tokenize(\n",
    "    sample1,\n",
    "    add_special_tokens=False,\n",
    "    max_length=20,\n",
    "    padding='max_length'\n",
    ")\n",
    "print(tokenized_text)\n",
    "\n",
    "input_ids = bert_tokenizer.encode(\n",
    "    sample1,\n",
    "    add_special_tokens=False,\n",
    "    max_length=20,\n",
    "    padding='max_length'\n",
    ")\n",
    "print(input_ids)\n",
    "\n",
    "decoded_ids = bert_tokenizer.decode(input_ids)\n",
    "print(decoded_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## í•œêµ­ì–´ Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "kor_text = 'ì•„ë²„ì§€ ê°€ë°©ì— ë“¤ì–´ê°€ì‹ ë‹¤'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[UNK]', '[UNK]', '[UNK]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n"
     ]
    }
   ],
   "source": [
    "tokenized_text = bert_tokenizer.tokenize(\n",
    "    kor_text,\n",
    "    add_special_tokens=False,\n",
    "    max_length=20,\n",
    "    padding='max_length'\n",
    ")\n",
    "print(tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids: \n",
      "\ttensor([[101, 100, 100, 100, 102]])\n",
      "token_type_ids: \n",
      "\ttensor([[0, 0, 0, 0, 0]])\n",
      "attention_mask: \n",
      "\ttensor([[1, 1, 1, 1, 1]])\n"
     ]
    }
   ],
   "source": [
    "tokenized_input_text = bert_tokenizer(kor_text, return_tensors='pt')\n",
    "for key, value in tokenized_input_text.items():\n",
    "    print(f'{key}: \\n\\t{value}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[UNK]', '[UNK]', '[UNK]']\n",
      "[101, 100, 100, 100, 102]\n",
      "[CLS] [UNK] [UNK] [UNK] [SEP]\n"
     ]
    }
   ],
   "source": [
    "tokenized_text = bert_tokenizer.tokenize(kor_text)\n",
    "print(tokenized_text)\n",
    "\n",
    "input_ids = bert_tokenizer.encode(kor_text)\n",
    "print(input_ids)\n",
    "\n",
    "decoded_ids = bert_tokenizer.decode(input_ids)\n",
    "print(decoded_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "MULTI_BERT_MODEL_NAME = 'bert-base-multilingual-cased'\n",
    "\n",
    "multi_bert_model = AutoModel.from_pretrained(MULTI_BERT_MODEL_NAME)\n",
    "multi_bert_tokenizer = AutoTokenizer.from_pretrained(MULTI_BERT_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "unk_text = 'í•œê¾ºì¸ ëœ°ë§Œ ì•Œì•„ë½ˆ ì‘¤ ìˆê¼ ì§ì½í•˜ê¼¤ì”ë‹ˆë”°'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[UNK]', '[UNK]', '[UNK]', 'ì‘¤', '[UNK]', '[UNK]']\n",
      "[100, 100, 100, 9510, 100, 100]\n",
      "[UNK] [UNK] [UNK] ì‘¤ [UNK] [UNK]\n"
     ]
    }
   ],
   "source": [
    "tokenized_text = multi_bert_tokenizer.tokenize(unk_text, add_special_tokens=False)\n",
    "print(tokenized_text)\n",
    "\n",
    "input_ids = multi_bert_tokenizer.encode(unk_text, add_special_tokens=False)\n",
    "print(input_ids)\n",
    "\n",
    "decoded_ids = multi_bert_tokenizer.decode(input_ids)\n",
    "print(decoded_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "['í•œê¾ºì¸', 'ëœ°ë§Œ', 'ì•Œì•„ë½ˆ', 'ì‘¤', 'ìˆê¼', 'ì§ì½í•˜ê¼¤ì”ë‹ˆë”°']\n",
      "[119547, 119548, 119549, 9510, 119550, 119551]\n",
      "í•œê¾ºì¸ ëœ°ë§Œ ì•Œì•„ë½ˆ ì‘¤ ìˆê¼ ì§ì½í•˜ê¼¤ì”ë‹ˆë”°\n"
     ]
    }
   ],
   "source": [
    "added_token_num = multi_bert_tokenizer.add_tokens(['í•œê¾ºì¸', 'ëœ°ë§Œ', 'ì•Œì•„ë½ˆ', 'ìˆê¼', 'ì§ì½í•˜ê¼¤ì”ë‹ˆë”°'])\n",
    "print(added_token_num)\n",
    "\n",
    "tokenized_text = multi_bert_tokenizer.tokenize(unk_text, add_special_tokens=False)\n",
    "print(tokenized_text)\n",
    "\n",
    "input_ids = multi_bert_tokenizer.encode(unk_text, add_special_tokens=False)\n",
    "print(input_ids)\n",
    "\n",
    "decoded_ids = multi_bert_tokenizer.decode(input_ids)\n",
    "print(decoded_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[', 'DA', '##D', ']', 'ì•„', '##ë¹ ', '[', '/', 'DA', '##D', ']', 'ê°€', 'ë°©', '##ì—', 'ë“¤ì–´', '##ê°€', '##ì‹ ', '##ë‹¤']\n",
      "[164, 47855, 11490, 166, 9519, 119008, 164, 120, 47855, 11490, 166, 8843, 9328, 10530, 71568, 11287, 25387, 11903]\n",
      "[ DAD ] ì•„ë¹  [ / DAD ] ê°€ ë°©ì— ë“¤ì–´ê°€ì‹ ë‹¤\n"
     ]
    }
   ],
   "source": [
    "special_token_text = '[DAD]ì•„ë¹ [/DAD]ê°€ ë°©ì— ë“¤ì–´ê°€ì‹ ë‹¤'\n",
    "\n",
    "tokenized_text = multi_bert_tokenizer.tokenize(special_token_text, add_special_tokens=False)\n",
    "print(tokenized_text)\n",
    "\n",
    "input_ids = multi_bert_tokenizer.encode(special_token_text, add_special_tokens=False)\n",
    "print(input_ids)\n",
    "\n",
    "decoded_ids = multi_bert_tokenizer.decode(input_ids)\n",
    "print(decoded_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[DAD]', 'ì•„', '##ë¹ ', '[/DAD]', 'ê°€', 'ë°©', '##ì—', 'ë“¤ì–´', '##ê°€', '##ì‹ ', '##ë‹¤']\n",
      "[119552, 9519, 119008, 119553, 8843, 9328, 10530, 71568, 11287, 25387, 11903]\n",
      "[DAD] ì•„ë¹  [/DAD] ê°€ ë°©ì— ë“¤ì–´ê°€ì‹ ë‹¤\n"
     ]
    }
   ],
   "source": [
    "special_token_text = '[DAD]ì•„ë¹ [/DAD]ê°€ ë°©ì— ë“¤ì–´ê°€ì‹ ë‹¤'\n",
    "\n",
    "added_token_num = multi_bert_tokenizer.add_special_tokens({'additional_special_tokens': ['[DAD]', '[/DAD]']})\n",
    "\n",
    "tokenized_text = multi_bert_tokenizer.tokenize(special_token_text, add_special_tokens=False)\n",
    "print(tokenized_text)\n",
    "\n",
    "input_ids = multi_bert_tokenizer.encode(special_token_text, add_special_tokens=False)\n",
    "print(input_ids)\n",
    "\n",
    "decoded_ids = multi_bert_tokenizer.decode(input_ids)\n",
    "print(decoded_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì•„ë¹  ê°€ ë°©ì— ë“¤ì–´ê°€ì‹ ë‹¤\n"
     ]
    }
   ],
   "source": [
    "decoded_ids = multi_bert_tokenizer.decode(input_ids, skip_special_tokens=True)\n",
    "print(decoded_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens (int):       [101, 9519, 119008, 11287, 9328, 10530, 71568, 11287, 25387, 11903, 102, 0, 0]\n",
      "Tokens (str):       [['[CLS]', 'ì•„', '##ë¹ ', '##ê°€', 'ë°©', '##ì—', 'ë“¤ì–´', '##ê°€', '##ì‹ ', '##ë‹¤', '[SEP]', '[PAD]', '[PAD]'], ['[CLS]', '[DAD]', 'ì•„', '##ë¹ ', '[/DAD]', 'ê°€', '##ë°©', '##ì—', '##ë“¤ì–´', '##ê°€', '##ì‹ ', '##ë‹¤', '[SEP]']]\n",
      "Tokens (attn_mask): [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0]\n",
      "\n",
      "Tokens (int):       [101, 119552, 9519, 119008, 119553, 8843, 42337, 10530, 93200, 11287, 25387, 11903, 102]\n",
      "Tokens (str):       [['[CLS]', 'ì•„', '##ë¹ ', '##ê°€', 'ë°©', '##ì—', 'ë“¤ì–´', '##ê°€', '##ì‹ ', '##ë‹¤', '[SEP]', '[PAD]', '[PAD]'], ['[CLS]', '[DAD]', 'ì•„', '##ë¹ ', '[/DAD]', 'ê°€', '##ë°©', '##ì—', '##ë“¤ì–´', '##ê°€', '##ì‹ ', '##ë‹¤', '[SEP]']]\n",
      "Tokens (attn_mask): [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample_list = [\n",
    "    'ì•„ë¹ ê°€ ë°©ì— ë“¤ì–´ê°€ì‹ ë‹¤',\n",
    "    '[DAD]ì•„ë¹ [/DAD]ê°€ë°©ì—ë“¤ì–´ê°€ì‹ ë‹¤'\n",
    "]\n",
    "\n",
    "tokens = multi_bert_tokenizer(\n",
    "    sample_list,\n",
    "    padding=True\n",
    ")\n",
    "\n",
    "for i in range(2):\n",
    "    print(f'Tokens (int):       {tokens['input_ids'][i]}')\n",
    "    print(f'Tokens (str):       {[multi_bert_tokenizer.convert_ids_to_tokens(s) for s in tokens['input_ids']]}')\n",
    "    print(f'Tokens (attn_mask): {tokens['attention_mask'][i]}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [MASK] í† í° ì˜ˆì¸¡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ì•„', '##ë¹ ', '##ê°€', '[MASK]', 'ë“¤ì–´', '##ê°€', '##ì‹ ', '##ë‹¤']\n"
     ]
    }
   ],
   "source": [
    "masked_text = 'ì•„ë¹ ê°€ [MASK] ë“¤ì–´ê°€ì‹ ë‹¤'\n",
    "tokenized_text = multi_bert_tokenizer.tokenize(masked_text)\n",
    "\n",
    "print(tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertForMaskedLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From ğŸ‘‰v4.50ğŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'score': 0.05300595983862877,\n",
       "  'token': 9654,\n",
       "  'token_str': 'ì˜',\n",
       "  'sequence': 'ì•„ë¹ ê°€ ì˜ ë“¤ì–´ê°€ì‹ ë‹¤'},\n",
       " {'score': 0.05062916874885559,\n",
       "  'token': 11287,\n",
       "  'token_str': '##ê°€',\n",
       "  'sequence': 'ì•„ë¹ ê°€ê°€ ë“¤ì–´ê°€ì‹ ë‹¤'},\n",
       " {'score': 0.047388385981321335,\n",
       "  'token': 8982,\n",
       "  'token_str': 'ë‚˜',\n",
       "  'sequence': 'ì•„ë¹ ê°€ ë‚˜ ë“¤ì–´ê°€ì‹ ë‹¤'},\n",
       " {'score': 0.03328689560294151,\n",
       "  'token': 9056,\n",
       "  'token_str': 'ë‹¤',\n",
       "  'sequence': 'ì•„ë¹ ê°€ ë‹¤ ë“¤ì–´ê°€ì‹ ë‹¤'},\n",
       " {'score': 0.026803359389305115,\n",
       "  'token': 14867,\n",
       "  'token_str': '##ë©´',\n",
       "  'sequence': 'ì•„ë¹ ê°€ë©´ ë“¤ì–´ê°€ì‹ ë‹¤'}]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "nlp_fill = pipeline('fill-mask', model=MULTI_BERT_MODEL_NAME)\n",
    "nlp_fill(masked_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### í•œêµ­ì–´ ì „ìš© ëª¨ë¸ì´ ì•„ë‹Œ multi-lingual BERTë¥¼ í™œìš©í–ˆë”ë‹ˆ ì•„ì‰¬ìš´ ê²°ê³¼ê°€ ë‚˜íƒ€ë‚œë‹¤ .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ì˜ì–´ ë¬¸ì¥ì—ì„œ [MASK] í† í° ì˜ˆì¸¡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Leave', 'before', 'you', '[MASK]', 'me']\n"
     ]
    }
   ],
   "source": [
    "masked_text = 'Leave before you [MASK] me'\n",
    "tokenized_text = multi_bert_tokenizer.tokenize(masked_text)\n",
    "\n",
    "print(tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'score': 0.3139025568962097,\n",
       "  'token': 11248,\n",
       "  'token_str': 'Love',\n",
       "  'sequence': 'Leave before you Love me'},\n",
       " {'score': 0.04809042811393738,\n",
       "  'token': 16138,\n",
       "  'token_str': 'love',\n",
       "  'sequence': 'Leave before you love me'},\n",
       " {'score': 0.04090191423892975,\n",
       "  'token': 10169,\n",
       "  'token_str': 'with',\n",
       "  'sequence': 'Leave before you with me'},\n",
       " {'score': 0.035010598599910736,\n",
       "  'token': 99401,\n",
       "  'token_str': 'Loved',\n",
       "  'sequence': 'Leave before you Loved me'},\n",
       " {'score': 0.030007442459464073,\n",
       "  'token': 10135,\n",
       "  'token_str': 'on',\n",
       "  'sequence': 'Leave before you on me'}]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "nlp_fill = pipeline('fill-mask', model=MULTI_BERT_MODEL_NAME)\n",
    "nlp_fill(masked_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
