{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer, BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BERT_MODEL_NAME = 'bert-base-cased'\n",
    "\n",
    "bert_model = AutoModel.from_pretrained(BERT_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tokenizer = AutoTokenizer.from_pretrained(BERT_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28996\n"
     ]
    }
   ],
   "source": [
    "print(bert_tokenizer.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dimensional\n",
      "Regiment\n",
      "##ING\n",
      "##atus\n",
      "##graphical\n",
      "1910\n",
      "else\n",
      "hardened\n",
      "##uku\n",
      "Football\n",
      "tickets\n",
      "tiger\n"
     ]
    }
   ],
   "source": [
    "for i, key in enumerate(bert_tokenizer.get_vocab()):\n",
    "    print(key)\n",
    "    if i > 10: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.models.bert.tokenization_bert_fast.BertTokenizerFast"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(bert_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample1 = 'I am interested in data science'\n",
    "sample2 = 'Iaminterestedindatascience'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids: \n",
      "\ttensor([[ 101,  146, 1821, 3888, 1107, 2233, 2598,  102]])\n",
      "token_type_ids: \n",
      "\ttensor([[0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "attention_mask: \n",
      "\ttensor([[1, 1, 1, 1, 1, 1, 1, 1]])\n"
     ]
    }
   ],
   "source": [
    "tokenized_input_text = bert_tokenizer(sample1, return_tensors='pt')\n",
    "for key, value in tokenized_input_text.items():\n",
    "    print(f'{key}: \\n\\t{value}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids: \n",
      "\ttensor([[  101,   146, 11787, 22456, 24732, 20344, 10401, 25982,  3633,   102]])\n",
      "token_type_ids: \n",
      "\ttensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "attention_mask: \n",
      "\ttensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n"
     ]
    }
   ],
   "source": [
    "tokenized_input_text_merged = bert_tokenizer(sample2, return_tensors='pt')\n",
    "for key, value in tokenized_input_text_merged.items():\n",
    "    print(f'{key}: \\n\\t{value}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 101,  146, 1821, 3888, 1107, 2233, 2598,  102]])\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1]])\n"
     ]
    }
   ],
   "source": [
    "# print(tokenized_input_text['input_ids'])\n",
    "print(tokenized_input_text.input_ids)\n",
    "\n",
    "# print(tokenized_input_text['token_type_ids'])\n",
    "print(tokenized_input_text.token_type_ids)\n",
    "\n",
    "# print(tokenized_input_text['attention_mask'])\n",
    "print(tokenized_input_text.attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'am', 'interested', 'in', 'data', 'science']\n",
      "[101, 146, 1821, 3888, 1107, 2233, 2598, 102]\n",
      "[CLS] I am interested in data science [SEP]\n"
     ]
    }
   ],
   "source": [
    "tokenized_text = bert_tokenizer.tokenize(sample1)\n",
    "print(tokenized_text)\n",
    "\n",
    "input_ids = bert_tokenizer.encode(sample1)\n",
    "print(input_ids)\n",
    "\n",
    "decoded_ids = bert_tokenizer.decode(input_ids)\n",
    "print(decoded_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'am', 'interested', 'in', 'data', 'science']\n",
      "[146, 1821, 3888, 1107, 2233, 2598]\n",
      "I am interested in data science\n"
     ]
    }
   ],
   "source": [
    "tokenized_text = bert_tokenizer.tokenize(sample1, add_special_tokens=False)\n",
    "print(tokenized_text)\n",
    "\n",
    "input_ids = bert_tokenizer.encode(sample1, add_special_tokens=False)\n",
    "print(input_ids)\n",
    "\n",
    "decoded_ids = bert_tokenizer.decode(input_ids)\n",
    "print(decoded_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'am', 'interested', 'in', 'data']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_text = bert_tokenizer.tokenize(\n",
    "    sample1,\n",
    "    add_special_tokens=False,\n",
    "    max_length=5,\n",
    "    truncation=True\n",
    ")\n",
    "tokenized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([146, 1821, 3888, 1107, 2233], 'I am interested in data')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids = bert_tokenizer.encode(\n",
    "    sample1,\n",
    "    add_special_tokens=False,\n",
    "    max_length=5,\n",
    "    truncation=True\n",
    ")\n",
    "decoded_ids = bert_tokenizer.decode(input_ids)\n",
    "input_ids, decoded_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PAD]\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(bert_tokenizer.pad_token)\n",
    "print(bert_tokenizer.pad_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'am', 'interested', 'in', 'data', 'science', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "[146, 1821, 3888, 1107, 2233, 2598, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "I am interested in data science [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n"
     ]
    }
   ],
   "source": [
    "tokenized_text = bert_tokenizer.tokenize(\n",
    "    sample1,\n",
    "    add_special_tokens=False,\n",
    "    max_length=20,\n",
    "    padding='max_length'\n",
    ")\n",
    "print(tokenized_text)\n",
    "\n",
    "input_ids = bert_tokenizer.encode(\n",
    "    sample1,\n",
    "    add_special_tokens=False,\n",
    "    max_length=20,\n",
    "    padding='max_length'\n",
    ")\n",
    "print(input_ids)\n",
    "\n",
    "decoded_ids = bert_tokenizer.decode(input_ids)\n",
    "print(decoded_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 한국어 Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "kor_text = '아버지 가방에 들어가신다'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[UNK]', '[UNK]', '[UNK]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n"
     ]
    }
   ],
   "source": [
    "tokenized_text = bert_tokenizer.tokenize(\n",
    "    kor_text,\n",
    "    add_special_tokens=False,\n",
    "    max_length=20,\n",
    "    padding='max_length'\n",
    ")\n",
    "print(tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids: \n",
      "\ttensor([[101, 100, 100, 100, 102]])\n",
      "token_type_ids: \n",
      "\ttensor([[0, 0, 0, 0, 0]])\n",
      "attention_mask: \n",
      "\ttensor([[1, 1, 1, 1, 1]])\n"
     ]
    }
   ],
   "source": [
    "tokenized_input_text = bert_tokenizer(kor_text, return_tensors='pt')\n",
    "for key, value in tokenized_input_text.items():\n",
    "    print(f'{key}: \\n\\t{value}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[UNK]', '[UNK]', '[UNK]']\n",
      "[101, 100, 100, 100, 102]\n",
      "[CLS] [UNK] [UNK] [UNK] [SEP]\n"
     ]
    }
   ],
   "source": [
    "tokenized_text = bert_tokenizer.tokenize(kor_text)\n",
    "print(tokenized_text)\n",
    "\n",
    "input_ids = bert_tokenizer.encode(kor_text)\n",
    "print(input_ids)\n",
    "\n",
    "decoded_ids = bert_tokenizer.decode(input_ids)\n",
    "print(decoded_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46e892c42ebe49489afb6b0aec3c9016",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/625 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\노승수\\.cache\\huggingface\\hub\\models--bert-base-multilingual-cased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5ee04d6fa154ece832d27a71eb08608",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/714M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df24538f47aa460c9f946bf79d7acaf2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b189203d1d3a45d39cc59883f5c19afa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/996k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e062030d79414f07b78c75c1f28188db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.96M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "MULTI_BERT_MODEL_NAME = 'bert-base-multilingual-cased'\n",
    "\n",
    "multi_bert_model = AutoModel.from_pretrained(MULTI_BERT_MODEL_NAME)\n",
    "multi_bert_tokenizer = AutoTokenizer.from_pretrained(MULTI_BERT_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "unk_text = '한꾺인 뜰만 알아뽈 쑤 있꼐 짞썽하꼤씁니따'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[UNK]', '[UNK]', '[UNK]', '쑤', '[UNK]', '[UNK]']\n",
      "[100, 100, 100, 9510, 100, 100]\n",
      "[UNK] [UNK] [UNK] 쑤 [UNK] [UNK]\n"
     ]
    }
   ],
   "source": [
    "tokenized_text = multi_bert_tokenizer.tokenize(unk_text, add_special_tokens=False)\n",
    "print(tokenized_text)\n",
    "\n",
    "input_ids = multi_bert_tokenizer.encode(unk_text, add_special_tokens=False)\n",
    "print(input_ids)\n",
    "\n",
    "decoded_ids = multi_bert_tokenizer.decode(input_ids)\n",
    "print(decoded_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "['한꾺인', '뜰만', '알아뽈', '쑤', '있꼐', '짞썽하꼤씁니따']\n",
      "[119547, 119548, 119549, 9510, 119550, 119551]\n",
      "한꾺인 뜰만 알아뽈 쑤 있꼐 짞썽하꼤씁니따\n"
     ]
    }
   ],
   "source": [
    "added_token_num = multi_bert_tokenizer.add_tokens(['한꾺인', '뜰만', '알아뽈', '있꼐', '짞썽하꼤씁니따'])\n",
    "print(added_token_num)\n",
    "\n",
    "tokenized_text = multi_bert_tokenizer.tokenize(unk_text, add_special_tokens=False)\n",
    "print(tokenized_text)\n",
    "\n",
    "input_ids = multi_bert_tokenizer.encode(unk_text, add_special_tokens=False)\n",
    "print(input_ids)\n",
    "\n",
    "decoded_ids = multi_bert_tokenizer.decode(input_ids)\n",
    "print(decoded_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[', 'DA', '##D', ']', '아', '##빠', '[', '/', 'DA', '##D', ']', '가', '방', '##에', '들어', '##가', '##신', '##다']\n",
      "[164, 47855, 11490, 166, 9519, 119008, 164, 120, 47855, 11490, 166, 8843, 9328, 10530, 71568, 11287, 25387, 11903]\n",
      "[ DAD ] 아빠 [ / DAD ] 가 방에 들어가신다\n"
     ]
    }
   ],
   "source": [
    "special_token_text = '[DAD]아빠[/DAD]가 방에 들어가신다'\n",
    "\n",
    "tokenized_text = multi_bert_tokenizer.tokenize(special_token_text, add_special_tokens=False)\n",
    "print(tokenized_text)\n",
    "\n",
    "input_ids = multi_bert_tokenizer.encode(special_token_text, add_special_tokens=False)\n",
    "print(input_ids)\n",
    "\n",
    "decoded_ids = multi_bert_tokenizer.decode(input_ids)\n",
    "print(decoded_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[DAD]', '아', '##빠', '[/DAD]', '가', '방', '##에', '들어', '##가', '##신', '##다']\n",
      "[119552, 9519, 119008, 119553, 8843, 9328, 10530, 71568, 11287, 25387, 11903]\n",
      "[DAD] 아빠 [/DAD] 가 방에 들어가신다\n"
     ]
    }
   ],
   "source": [
    "special_token_text = '[DAD]아빠[/DAD]가 방에 들어가신다'\n",
    "\n",
    "added_token_num = multi_bert_tokenizer.add_special_tokens({'additional_special_tokens': ['[DAD]', '[/DAD]']})\n",
    "\n",
    "tokenized_text = multi_bert_tokenizer.tokenize(special_token_text, add_special_tokens=False)\n",
    "print(tokenized_text)\n",
    "\n",
    "input_ids = multi_bert_tokenizer.encode(special_token_text, add_special_tokens=False)\n",
    "print(input_ids)\n",
    "\n",
    "decoded_ids = multi_bert_tokenizer.decode(input_ids)\n",
    "print(decoded_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "아빠 가 방에 들어가신다\n"
     ]
    }
   ],
   "source": [
    "decoded_ids = multi_bert_tokenizer.decode(input_ids, skip_special_tokens=True)\n",
    "print(decoded_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens (int):       [101, 9519, 119008, 11287, 9328, 10530, 71568, 11287, 25387, 11903, 102, 0, 0]\n",
      "Tokens (str):       [['[CLS]', '아', '##빠', '##가', '방', '##에', '들어', '##가', '##신', '##다', '[SEP]', '[PAD]', '[PAD]'], ['[CLS]', '[DAD]', '아', '##빠', '[/DAD]', '가', '##방', '##에', '##들어', '##가', '##신', '##다', '[SEP]']]\n",
      "Tokens (attn_mask): [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0]\n",
      "\n",
      "Tokens (int):       [101, 119552, 9519, 119008, 119553, 8843, 42337, 10530, 93200, 11287, 25387, 11903, 102]\n",
      "Tokens (str):       [['[CLS]', '아', '##빠', '##가', '방', '##에', '들어', '##가', '##신', '##다', '[SEP]', '[PAD]', '[PAD]'], ['[CLS]', '[DAD]', '아', '##빠', '[/DAD]', '가', '##방', '##에', '##들어', '##가', '##신', '##다', '[SEP]']]\n",
      "Tokens (attn_mask): [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample_list = [\n",
    "    '아빠가 방에 들어가신다',\n",
    "    '[DAD]아빠[/DAD]가방에들어가신다'\n",
    "]\n",
    "\n",
    "tokens = multi_bert_tokenizer(\n",
    "    sample_list,\n",
    "    padding=True\n",
    ")\n",
    "\n",
    "for i in range(2):\n",
    "    print(f'Tokens (int):       {tokens['input_ids'][i]}')\n",
    "    print(f'Tokens (str):       {[multi_bert_tokenizer.convert_ids_to_tokens(s) for s in tokens['input_ids']]}')\n",
    "    print(f'Tokens (attn_mask): {tokens['attention_mask'][i]}')\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
