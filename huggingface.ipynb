{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer, BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "BERT_MODEL_NAME = 'bert-base-cased'\n",
    "\n",
    "bert_model = AutoModel.from_pretrained(BERT_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tokenizer = AutoTokenizer.from_pretrained(BERT_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28996\n"
     ]
    }
   ],
   "source": [
    "print(bert_tokenizer.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##arded\n",
      "##ations\n",
      "controlled\n",
      "exceptions\n",
      "##zzy\n",
      "headlines\n",
      "Carr\n",
      "##sko\n",
      "Vita\n",
      "paused\n",
      "√\n",
      "2019\n"
     ]
    }
   ],
   "source": [
    "for i, key in enumerate(bert_tokenizer.get_vocab()):\n",
    "    print(key)\n",
    "    if i > 10: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.models.bert.tokenization_bert_fast.BertTokenizerFast"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(bert_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample1 = 'I am interested in data science'\n",
    "sample2 = 'Iaminterestedindatascience'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids: \n",
      "\ttensor([[ 101,  146, 1821, 3888, 1107, 2233, 2598,  102]])\n",
      "token_type_ids: \n",
      "\ttensor([[0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "attention_mask: \n",
      "\ttensor([[1, 1, 1, 1, 1, 1, 1, 1]])\n"
     ]
    }
   ],
   "source": [
    "tokenized_input_text = bert_tokenizer(sample1, return_tensors='pt')\n",
    "for key, value in tokenized_input_text.items():\n",
    "    print(f'{key}: \\n\\t{value}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids: \n",
      "\ttensor([[  101,   146, 11787, 22456, 24732, 20344, 10401, 25982,  3633,   102]])\n",
      "token_type_ids: \n",
      "\ttensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "attention_mask: \n",
      "\ttensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n"
     ]
    }
   ],
   "source": [
    "tokenized_input_text_merged = bert_tokenizer(sample2, return_tensors='pt')\n",
    "for key, value in tokenized_input_text_merged.items():\n",
    "    print(f'{key}: \\n\\t{value}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 101,  146, 1821, 3888, 1107, 2233, 2598,  102]])\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1]])\n"
     ]
    }
   ],
   "source": [
    "# print(tokenized_input_text['input_ids'])\n",
    "print(tokenized_input_text.input_ids)\n",
    "\n",
    "# print(tokenized_input_text['token_type_ids'])\n",
    "print(tokenized_input_text.token_type_ids)\n",
    "\n",
    "# print(tokenized_input_text['attention_mask'])\n",
    "print(tokenized_input_text.attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'am', 'interested', 'in', 'data', 'science']\n",
      "[101, 146, 1821, 3888, 1107, 2233, 2598, 102]\n",
      "[CLS] I am interested in data science [SEP]\n"
     ]
    }
   ],
   "source": [
    "tokenized_text = bert_tokenizer.tokenize(sample1)\n",
    "print(tokenized_text)\n",
    "\n",
    "input_ids = bert_tokenizer.encode(sample1)\n",
    "print(input_ids)\n",
    "\n",
    "decoded_ids = bert_tokenizer.decode(input_ids)\n",
    "print(decoded_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'am', 'interested', 'in', 'data', 'science']\n",
      "[146, 1821, 3888, 1107, 2233, 2598]\n",
      "I am interested in data science\n"
     ]
    }
   ],
   "source": [
    "tokenized_text = bert_tokenizer.tokenize(sample1, add_special_tokens=False)\n",
    "print(tokenized_text)\n",
    "\n",
    "input_ids = bert_tokenizer.encode(sample1, add_special_tokens=False)\n",
    "print(input_ids)\n",
    "\n",
    "decoded_ids = bert_tokenizer.decode(input_ids)\n",
    "print(decoded_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'am', 'interested', 'in', 'data']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_text = bert_tokenizer.tokenize(\n",
    "    sample1,\n",
    "    add_special_tokens=False,\n",
    "    max_length=5,\n",
    "    truncation=True\n",
    ")\n",
    "tokenized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([146, 1821, 3888, 1107, 2233], 'I am interested in data')"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids = bert_tokenizer.encode(\n",
    "    sample1,\n",
    "    add_special_tokens=False,\n",
    "    max_length=5,\n",
    "    truncation=True\n",
    ")\n",
    "decoded_ids = bert_tokenizer.decode(input_ids)\n",
    "input_ids, decoded_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PAD]\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(bert_tokenizer.pad_token)\n",
    "print(bert_tokenizer.pad_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'am', 'interested', 'in', 'data', 'science', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "[146, 1821, 3888, 1107, 2233, 2598, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "I am interested in data science [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n"
     ]
    }
   ],
   "source": [
    "tokenized_text = bert_tokenizer.tokenize(\n",
    "    sample1,\n",
    "    add_special_tokens=False,\n",
    "    max_length=20,\n",
    "    padding='max_length'\n",
    ")\n",
    "print(tokenized_text)\n",
    "\n",
    "input_ids = bert_tokenizer.encode(\n",
    "    sample1,\n",
    "    add_special_tokens=False,\n",
    "    max_length=20,\n",
    "    padding='max_length'\n",
    ")\n",
    "print(input_ids)\n",
    "\n",
    "decoded_ids = bert_tokenizer.decode(input_ids)\n",
    "print(decoded_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 한국어 Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "kor_text = '아버지 가방에 들어가신다'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[UNK]', '[UNK]', '[UNK]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n"
     ]
    }
   ],
   "source": [
    "tokenized_text = bert_tokenizer.tokenize(\n",
    "    kor_text,\n",
    "    add_special_tokens=False,\n",
    "    max_length=20,\n",
    "    padding='max_length'\n",
    ")\n",
    "print(tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids: \n",
      "\ttensor([[101, 100, 100, 100, 102]])\n",
      "token_type_ids: \n",
      "\ttensor([[0, 0, 0, 0, 0]])\n",
      "attention_mask: \n",
      "\ttensor([[1, 1, 1, 1, 1]])\n"
     ]
    }
   ],
   "source": [
    "tokenized_input_text = bert_tokenizer(kor_text, return_tensors='pt')\n",
    "for key, value in tokenized_input_text.items():\n",
    "    print(f'{key}: \\n\\t{value}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[UNK]', '[UNK]', '[UNK]']\n",
      "[101, 100, 100, 100, 102]\n",
      "[CLS] [UNK] [UNK] [UNK] [SEP]\n"
     ]
    }
   ],
   "source": [
    "tokenized_text = bert_tokenizer.tokenize(kor_text)\n",
    "print(tokenized_text)\n",
    "\n",
    "input_ids = bert_tokenizer.encode(kor_text)\n",
    "print(input_ids)\n",
    "\n",
    "decoded_ids = bert_tokenizer.decode(input_ids)\n",
    "print(decoded_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "MULTI_BERT_MODEL_NAME = 'bert-base-multilingual-cased'\n",
    "\n",
    "multi_bert_model = AutoModel.from_pretrained(MULTI_BERT_MODEL_NAME)\n",
    "multi_bert_tokenizer = AutoTokenizer.from_pretrained(MULTI_BERT_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "unk_text = '한꾺인 뜰만 알아뽈 쑤 있꼐 짞썽하꼤씁니따'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[UNK]', '[UNK]', '[UNK]', '쑤', '[UNK]', '[UNK]']\n",
      "[100, 100, 100, 9510, 100, 100]\n",
      "[UNK] [UNK] [UNK] 쑤 [UNK] [UNK]\n"
     ]
    }
   ],
   "source": [
    "tokenized_text = multi_bert_tokenizer.tokenize(unk_text, add_special_tokens=False)\n",
    "print(tokenized_text)\n",
    "\n",
    "input_ids = multi_bert_tokenizer.encode(unk_text, add_special_tokens=False)\n",
    "print(input_ids)\n",
    "\n",
    "decoded_ids = multi_bert_tokenizer.decode(input_ids)\n",
    "print(decoded_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "['한꾺인', '뜰만', '알아뽈', '쑤', '있꼐', '짞썽하꼤씁니따']\n",
      "[119547, 119548, 119549, 9510, 119550, 119551]\n",
      "한꾺인 뜰만 알아뽈 쑤 있꼐 짞썽하꼤씁니따\n"
     ]
    }
   ],
   "source": [
    "added_token_num = multi_bert_tokenizer.add_tokens(['한꾺인', '뜰만', '알아뽈', '있꼐', '짞썽하꼤씁니따'])\n",
    "print(added_token_num)\n",
    "\n",
    "tokenized_text = multi_bert_tokenizer.tokenize(unk_text, add_special_tokens=False)\n",
    "print(tokenized_text)\n",
    "\n",
    "input_ids = multi_bert_tokenizer.encode(unk_text, add_special_tokens=False)\n",
    "print(input_ids)\n",
    "\n",
    "decoded_ids = multi_bert_tokenizer.decode(input_ids)\n",
    "print(decoded_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[', 'DA', '##D', ']', '아', '##빠', '[', '/', 'DA', '##D', ']', '가', '방', '##에', '들어', '##가', '##신', '##다']\n",
      "[164, 47855, 11490, 166, 9519, 119008, 164, 120, 47855, 11490, 166, 8843, 9328, 10530, 71568, 11287, 25387, 11903]\n",
      "[ DAD ] 아빠 [ / DAD ] 가 방에 들어가신다\n"
     ]
    }
   ],
   "source": [
    "special_token_text = '[DAD]아빠[/DAD]가 방에 들어가신다'\n",
    "\n",
    "tokenized_text = multi_bert_tokenizer.tokenize(special_token_text, add_special_tokens=False)\n",
    "print(tokenized_text)\n",
    "\n",
    "input_ids = multi_bert_tokenizer.encode(special_token_text, add_special_tokens=False)\n",
    "print(input_ids)\n",
    "\n",
    "decoded_ids = multi_bert_tokenizer.decode(input_ids)\n",
    "print(decoded_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[DAD]', '아', '##빠', '[/DAD]', '가', '방', '##에', '들어', '##가', '##신', '##다']\n",
      "[119552, 9519, 119008, 119553, 8843, 9328, 10530, 71568, 11287, 25387, 11903]\n",
      "[DAD] 아빠 [/DAD] 가 방에 들어가신다\n"
     ]
    }
   ],
   "source": [
    "special_token_text = '[DAD]아빠[/DAD]가 방에 들어가신다'\n",
    "\n",
    "added_token_num = multi_bert_tokenizer.add_special_tokens({'additional_special_tokens': ['[DAD]', '[/DAD]']})\n",
    "\n",
    "tokenized_text = multi_bert_tokenizer.tokenize(special_token_text, add_special_tokens=False)\n",
    "print(tokenized_text)\n",
    "\n",
    "input_ids = multi_bert_tokenizer.encode(special_token_text, add_special_tokens=False)\n",
    "print(input_ids)\n",
    "\n",
    "decoded_ids = multi_bert_tokenizer.decode(input_ids)\n",
    "print(decoded_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "아빠 가 방에 들어가신다\n"
     ]
    }
   ],
   "source": [
    "decoded_ids = multi_bert_tokenizer.decode(input_ids, skip_special_tokens=True)\n",
    "print(decoded_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens (int):       [101, 9519, 119008, 11287, 9328, 10530, 71568, 11287, 25387, 11903, 102, 0, 0]\n",
      "Tokens (str):       [['[CLS]', '아', '##빠', '##가', '방', '##에', '들어', '##가', '##신', '##다', '[SEP]', '[PAD]', '[PAD]'], ['[CLS]', '[DAD]', '아', '##빠', '[/DAD]', '가', '##방', '##에', '##들어', '##가', '##신', '##다', '[SEP]']]\n",
      "Tokens (attn_mask): [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0]\n",
      "\n",
      "Tokens (int):       [101, 119552, 9519, 119008, 119553, 8843, 42337, 10530, 93200, 11287, 25387, 11903, 102]\n",
      "Tokens (str):       [['[CLS]', '아', '##빠', '##가', '방', '##에', '들어', '##가', '##신', '##다', '[SEP]', '[PAD]', '[PAD]'], ['[CLS]', '[DAD]', '아', '##빠', '[/DAD]', '가', '##방', '##에', '##들어', '##가', '##신', '##다', '[SEP]']]\n",
      "Tokens (attn_mask): [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample_list = [\n",
    "    '아빠가 방에 들어가신다',\n",
    "    '[DAD]아빠[/DAD]가방에들어가신다'\n",
    "]\n",
    "\n",
    "tokens = multi_bert_tokenizer(\n",
    "    sample_list,\n",
    "    padding=True\n",
    ")\n",
    "\n",
    "for i in range(2):\n",
    "    print(f'Tokens (int):       {tokens['input_ids'][i]}')\n",
    "    print(f'Tokens (str):       {[multi_bert_tokenizer.convert_ids_to_tokens(s) for s in tokens['input_ids']]}')\n",
    "    print(f'Tokens (attn_mask): {tokens['attention_mask'][i]}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [MASK] 토큰 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['아', '##빠', '##가', '[MASK]', '들어', '##가', '##신', '##다']\n"
     ]
    }
   ],
   "source": [
    "masked_text = '아빠가 [MASK] 들어가신다'\n",
    "tokenized_text = multi_bert_tokenizer.tokenize(masked_text)\n",
    "\n",
    "print(tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertForMaskedLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'score': 0.05300595983862877,\n",
       "  'token': 9654,\n",
       "  'token_str': '잘',\n",
       "  'sequence': '아빠가 잘 들어가신다'},\n",
       " {'score': 0.05062916874885559,\n",
       "  'token': 11287,\n",
       "  'token_str': '##가',\n",
       "  'sequence': '아빠가가 들어가신다'},\n",
       " {'score': 0.047388385981321335,\n",
       "  'token': 8982,\n",
       "  'token_str': '나',\n",
       "  'sequence': '아빠가 나 들어가신다'},\n",
       " {'score': 0.03328689560294151,\n",
       "  'token': 9056,\n",
       "  'token_str': '다',\n",
       "  'sequence': '아빠가 다 들어가신다'},\n",
       " {'score': 0.026803359389305115,\n",
       "  'token': 14867,\n",
       "  'token_str': '##면',\n",
       "  'sequence': '아빠가면 들어가신다'}]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "nlp_fill = pipeline('fill-mask', model=MULTI_BERT_MODEL_NAME)\n",
    "nlp_fill(masked_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 한국어 전용 모델이 아닌 multi-lingual BERT를 활용했더니 아쉬운 결과가 나타난다 .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 영어 문장에서 [MASK] 토큰 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Leave', 'before', 'you', '[MASK]', 'me']\n"
     ]
    }
   ],
   "source": [
    "masked_text = 'Leave before you [MASK] me'\n",
    "tokenized_text = multi_bert_tokenizer.tokenize(masked_text)\n",
    "\n",
    "print(tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'score': 0.3139025568962097,\n",
       "  'token': 11248,\n",
       "  'token_str': 'Love',\n",
       "  'sequence': 'Leave before you Love me'},\n",
       " {'score': 0.04809042811393738,\n",
       "  'token': 16138,\n",
       "  'token_str': 'love',\n",
       "  'sequence': 'Leave before you love me'},\n",
       " {'score': 0.04090191423892975,\n",
       "  'token': 10169,\n",
       "  'token_str': 'with',\n",
       "  'sequence': 'Leave before you with me'},\n",
       " {'score': 0.035010598599910736,\n",
       "  'token': 99401,\n",
       "  'token_str': 'Loved',\n",
       "  'sequence': 'Leave before you Loved me'},\n",
       " {'score': 0.030007442459464073,\n",
       "  'token': 10135,\n",
       "  'token_str': 'on',\n",
       "  'sequence': 'Leave before you on me'}]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "nlp_fill = pipeline('fill-mask', model=MULTI_BERT_MODEL_NAME)\n",
    "nlp_fill(masked_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
